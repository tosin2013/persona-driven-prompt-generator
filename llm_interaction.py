import os
import logging
import litellm
import streamlit as st
import random
import time
from typing import List, Dict, Any

def configure_litellm() -> str:
    """
    Configure LiteLLM based on environment variables.

    Returns:
        str: The model name configured for LiteLLM.
    """
    model = os.getenv("LITELLM_MODEL")
    provider = os.getenv("LITELLM_PROVIDER")

    if not model or not provider:
        raise ValueError("LITELLM_MODEL and LITELLM_PROVIDER environment variables must be set")

    logging.debug(f"Configuring LiteLLM with model='{model}', provider='{provider}'")

    api_key_env_map = {
        "openai": "OPENAI_API_KEY",
        "groq": "GROQ_API_KEY",
        "deepseek": "DEEPSEEK_API_KEY",
        "huggingface": "HUGGINGFACE_API_KEY",
        "ollama": "OLLAMA_API_KEY",
        "mistral": "MISTRAL_API_KEY",
    }

    api_key_env = api_key_env_map.get(provider)
    if not api_key_env:
        raise ValueError(f"Unsupported provider: {provider}")

    api_key = os.getenv(api_key_env)
    if not api_key:
        raise ValueError(f"{api_key_env} environment variable not set")

    litellm.api_key = api_key
    return model

def generate_persona_prompt(persona: Dict[str, Any]) -> str:
    """
    Generate a prompt for a given persona.

    Args:
        persona (Dict[str, Any]): The persona dictionary.

    Returns:
        str: The generated prompt.
    """
    emotional_tone = persona.get('emotional_tone', 'neutral')
    return f"{persona['name']} ({emotional_tone}): {persona['goals']}"

def update_ui_with_message(persona_name: str, message: str) -> None:
    """
    Update the UI with a new message from a persona.

    Args:
        persona_name (str): The name of the persona.
        message (str): The message to display.
    """
    st.session_state.messages.append({"role": "system", "content": f"{persona_name}: {message}"})
    with st.chat_message("system"):
        st.markdown(f"{persona_name}: {message}")

def autochat(personas: List[Dict[str, Any]], num_users: int) -> None:
    """
    Simulates a chat between the personas using the LLM.

    Args:
        personas (List[Dict[str, Any]]): The list of personas.
        num_users (int): Number of users in the conversation.
    """
    model = configure_litellm()

    for i in range(10):  # Simulate 10 exchanges
        for _ in range(num_users):
            persona = random.choice(personas)
            prompt = generate_persona_prompt(persona)
            response = litellm.completion(
                model=model,
                messages=[{"role": "user", "content": prompt}],
                temperature=0.7
            )
            message = response.choices[0].message.content.strip()
            update_ui_with_message(persona['name'], message)
            time.sleep(1)  # Sleep for 1 second between responses

def interact_with_llm(prompt: str) -> str:
    """
    Interact with the LLM and return the response.

    Args:
        prompt (str): The prompt to send to the LLM.

    Returns:
        str: The response from the LLM.
    """
    model = configure_litellm()
    response = litellm.completion(
        model=model,
        messages=[{"role": "user", "content": prompt}],
        temperature=0.7
    )
    return response.choices[0].message.content.strip()

def submit_prompt_to_llm(prompt: str, api_key: str = None, temperature: float = 0.7, max_tokens: int = 1024) -> str:
    """
    Submit a formatted prompt to LiteLLM and return the response.

    Args:
        prompt (str): The formatted prompt to send to LiteLLM.
        api_key (str): Optional API key for accessing the LLM. Can also use default in LiteLLM config.
        temperature (float): Sampling temperature to control randomness.
        max_tokens (int): Maximum number of tokens to include in the response.

    Returns:
        str: The response generated by the LLM.
    """
    model_name = os.getenv("LITELLM_MODEL", "gpt-3.5-turbo")  # Use environment variable or default to "gpt-3.5-turbo"
    
    # If using API keys dynamically
    config = {}
    if api_key:
        config["api_key"] = api_key

    try:
        logging.debug(f"Submitting prompt to LiteLLM with model='{model_name}', temperature={temperature}, max_tokens={max_tokens}")
        logging.debug(f"Prompt: {prompt}")
        # Submit the prompt to LiteLLM
        response = litellm.completion(
            model=model_name,
            messages=[{"role": "user", "content": prompt}],
            temperature=temperature,
            max_tokens=max_tokens,
            **config,
        )
        logging.debug(f"Response: {response}")

        # Extract the content from the response
        return response['choices'][0]['message']['content'].strip()
    except litellm.exceptions.APIError as e:
        logging.error(f"LiteLLM API Error: {e}")
        return f"An API error occurred: {e}"
    except litellm.exceptions.AuthenticationError as e:
        logging.error(f"LiteLLM Authentication Error: {e}")
        return f"An authentication error occurred: {e}"
    except litellm.exceptions.RateLimitError as e:
        logging.error(f"LiteLLM Rate Limit Error: {e}")
        return f"A rate limit error occurred: {e}"
    except Exception as e:
        logging.error(f"Unexpected error submitting prompt to LiteLLM: {e}")
        return f"An unexpected error occurred: {e}"
